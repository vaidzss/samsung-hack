
# NutriGuide: Local-First, AI-Powered Nutrient Tracking

**Team: PythonInStack**
* Vaidic Srivastava
* Vatts Kumar Mishra
* Garvit Sisodia
  ### Video link
 [demo-link](https://youtu.be/cMfUXhTuquE?si=rDnX7GSOx95uRI6l)


---

## üìú Overview

**NutriGuide** is an intelligent nutrient tracking application that allows users to identify food items from images and receive detailed nutritional information. What sets NutriGuide apart is its commitment to **privacy and local-first processing**. All AI models run directly on the user's device, ensuring that personal data remains secure and is never sent to the cloud. This approach guarantees complete data ownership and offline functionality.

The application uses a sophisticated combination of fine-tuned AI models to recognize a wide variety of foods and provide insightful nutritional analysis, making healthy eating accessible and private.

---

## ‚ú® Core Features

* **üì∏ Image-Based Food Recognition:** Simply snap a photo of your meal, and NutriGuide's AI will identify the food items.
* **üìù Text-Based Logging:** Don't have a photo? Describe your meal in text to get the same detailed analysis.
* **üîí 100% Local & Private:** All AI processing happens on your device. Your photos and data never leave your machine.
* **üìä Detailed Nutritional Breakdown:** Get instant information on calories, protein, fats, and carbs for your meals.
* **üí° AI-Powered Insights:** Receive daily health tips and weekly summaries of your diet generated by a local language model.
* <b></b>üåó Light & Dark Mode:** A sleek, modern interface that's easy on the eyes, day or night.

---

## ü§ñ Models & Fine-Tuning

NutriGuide's intelligence is powered by two core models, both fine-tuned for the specific task of food recognition and analysis.

### 1. Vision Model: Fine-Tuned Google ViT

For image recognition, we started with a powerful base model and adapted it to specialize in food imagery.

* **Base Model:** [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
* **Fine-Tuning:** The base Vision Transformer (ViT) was fine-tuned on a comprehensive, custom-built dataset to accurately identify a wide range of food items, including specific cultural cuisines.
* **Fine-tuned model: [google-vit-basepack-finetuned](https://huggingface.co/vaidzs/google-vit-basepack-finetuned)

### 2. Text Model: Fine-Tuned TinyLlama

To provide intelligent suggestions and understand user queries, we fine-tuned a compact and efficient language model.

* **Base Model:** [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
* **Fine-Tuning:** We fine-tuned this model on the extensive list of food labels from our combined nutrition datasets. This process trained the model to recognize a vast vocabulary of food items, handle typos gracefully, and provide more accurate suggestions when a user searches for a food item via text.
* **Fine-Tuned: [Tinyllama1b_finetuned](https://huggingface.co/vaidzs/tinyllama1B_chat_finetuned)

---

## üìö Datasets

Our models' accuracy is built on a foundation of diverse, high-quality open-source datasets. We combined several sources to create a robust and comprehensive knowledge base for both food imagery and nutritional information.

### Image Datasets for ViT Fine-Tuning

* **Food-101 (Subset):** [rjstatic/food-101-subset on Hugging Face](https://huggingface.co/datasets/rjstatic/food-101-subset)
* **Food-101 (Full):** The classic, large-scale dataset containing 101,000 images of 101 food categories.
* **Indian Food Dataset (101):** [Indian Food Dataset (101) on Kaggle](https://www.kaggle.com/datasets/anshulmehtakaggl/indian-food-dataset-101)

### Nutritional Datasets for Database & LLM Fine-Tuning

* **Food and Nutrition Data:** [Food and Nutrition data on Kaggle](https://www.kaggle.com/datasets/shreyashs/food-and-nutrition-data)
* **Nutrition Facts for Over 8,000 Foods:** [USDA Nutrition Dataset on Kaggle](https://www.kaggle.com/datasets/niharika41298/nutrition-details-for-most-common-foods)

By **merging these datasets**, we created a single, powerful dataset that trained our ViT model to recognize a global variety of foods and provided the labels needed to make our TinyLlama model an expert in food-related terminology.

---

## üöÄ Tech Stack

* **Backend:** FastAPI, Python
* **Frontend:** React.js, Tailwind CSS
* **AI / ML:** PyTorch, Hugging Face Transformers
* **Core Libraries:** Pandas, Pillow, Uvicorn

---

## ‚öôÔ∏è Setup & Installation

To run this project locally, follow these steps:

### Backend Setup

1.  Navigate to the backend directory.
2.  Install the required Python packages:
    ```bash
    pip install -r requirements.txt
    ```
3.  Ensure you have the fine-tuned models and dataset files in the correct directories as specified in `main.py`.
4.  Run the FastAPI server:
    ```bash
    uvicorn main:app --reload
    ```
    The backend will be available at `http://127.0.0.1:8000`.

### Frontend Setup

1.  Navigate to the frontend directory.
2.  Install the necessary npm packages:
    ```bash
    npm install
    ```
3.  Start the React development server:
    ```bash
    npm run dev
    ```
    The application will be accessible in your browser, typically at `http://localhost:5173`.



=

